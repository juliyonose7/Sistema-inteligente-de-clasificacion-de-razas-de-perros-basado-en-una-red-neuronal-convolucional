"""
ğŸ• ENTRENADOR BINARIO CANINO - AMD 7800X3D OPTIMIZADO
Entrena modelo binario (perro vs no-perro) con mÃ¡ximo rendimiento
"""

import os
import sys
import time
import json
import threading
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from tqdm import tqdm
from torch.optim.lr_scheduler import OneCycleLR
import warnings
warnings.filterwarnings('ignore')

# Import especÃ­fico para Windows
if sys.platform == "win32":
    import msvcrt
else:
    import select

# Control de parada manual
class TrainingController:
    def __init__(self):
        self.should_stop = False
        self.input_thread = None
        self.monitoring = False
    
    def start_monitoring(self):
        """Inicia el monitoreo de input del usuario"""
        self.monitoring = True
        self.should_stop = False
        self.input_thread = threading.Thread(target=self._monitor_input, daemon=True)
        self.input_thread.start()
        print("ğŸ”„ CONTROL DE ENTRENAMIENTO ACTIVADO")
        print("   Presiona 'q' + Enter para parar el entrenamiento de forma segura")
        print("   Presiona 's' + Enter para mostrar estadÃ­sticas")
        print("=" * 70)
    
    def _monitor_input(self):
        """Monitorea el input del usuario en un hilo separado"""
        while self.monitoring:
            try:
                if sys.platform == "win32":
                    import msvcrt
                    if msvcrt.kbhit():
                        key = msvcrt.getch().decode('utf-8').lower()
                        if key == 'q':
                            print("\nğŸ›‘ PARADA SOLICITADA - Terminando Ã©poca actual de forma segura...")
                            self.should_stop = True
                            break
                        elif key == 's':
                            print(f"\nğŸ“Š STATUS: Entrenamiento en progreso... (Presiona 'q' para parar)")
                else:
                    # Para sistemas Unix/Linux
                    import select
                    if select.select([sys.stdin], [], [], 0.1)[0]:
                        key = sys.stdin.readline().strip().lower()
                        if key == 'q':
                            print("\nğŸ›‘ PARADA SOLICITADA - Terminando Ã©poca actual de forma segura...")
                            self.should_stop = True
                            break
                        elif key == 's':
                            print(f"\nğŸ“Š STATUS: Entrenamiento en progreso... (Presiona 'q' para parar)")
                
                time.sleep(0.1)
            except:
                time.sleep(0.1)
                continue
    
    def stop_monitoring(self):
        """Detiene el monitoreo"""
        self.monitoring = False
        if self.input_thread and self.input_thread.is_alive():
            self.input_thread.join(timeout=1.0)

# ===================================================================
# CONFIGURACIÃ“N AMD 7800X3D
# ===================================================================
def optimize_for_7800x3d():
    """Optimizaciones especÃ­ficas para AMD 7800X3D"""
    os.environ['MKL_NUM_THREADS'] = '16'
    os.environ['NUMEXPR_NUM_THREADS'] = '16'
    os.environ['OMP_NUM_THREADS'] = '16'
    os.environ['OPENBLAS_NUM_THREADS'] = '16'
    os.environ['VECLIB_MAXIMUM_THREADS'] = '16'
    
    # Configurar PyTorch para mÃ¡ximo rendimiento en CPU
    torch.set_num_threads(16)
    torch.set_num_interop_threads(4)
    
    print("ğŸš€ Variables de entorno 7800X3D configuradas")
    print(f"ğŸ’» CPU threads: {torch.get_num_threads()}")

# ===================================================================
# DATASET BINARIO
# ===================================================================
class BinaryDogDataset(Dataset):
    """Dataset para clasificaciÃ³n binaria perro/no-perro"""
    
    def __init__(self, data_path, transform=None, max_samples_per_class=None):
        self.data_path = Path(data_path)
        self.transform = transform
        self.samples = []
        self.classes = ['no_dog', 'dog']  # 0: no-perro, 1: perro
        
        self._load_samples(max_samples_per_class)
        
    def _load_samples(self, max_samples_per_class):
        """Carga muestras balanceadas"""
        print("ğŸ”„ Cargando dataset binario...")
        
        # Cargar imÃ¡genes NO-PERRO
        nodog_path = self.data_path / "NODOG"
        if nodog_path.exists():
            nodog_count = 0
            
            # Archivos individuales
            for img_file in nodog_path.glob("*.jpg"):
                if max_samples_per_class and nodog_count >= max_samples_per_class:
                    break
                self.samples.append((str(img_file), 0))
                nodog_count += 1
                
            # Subdirectorios
            for subdir in nodog_path.iterdir():
                if subdir.is_dir():
                    for img_file in subdir.glob("*.jpg"):
                        if max_samples_per_class and nodog_count >= max_samples_per_class:
                            break
                        self.samples.append((str(img_file), 0))
                        nodog_count += 1
                        
            print(f"   âŒ NO-PERRO: {nodog_count:,} imÃ¡genes")
        
        # Cargar imÃ¡genes PERRO
        yesdog_path = self.data_path / "YESDOG"
        if yesdog_path.exists():
            dog_count = 0
            
            for breed_dir in yesdog_path.iterdir():
                if breed_dir.is_dir():
                    # Buscar tanto .JPEG como .jpg
                    for img_file in list(breed_dir.glob("*.JPEG")) + list(breed_dir.glob("*.jpg")):
                        if max_samples_per_class and dog_count >= max_samples_per_class:
                            break
                        self.samples.append((str(img_file), 1))
                        dog_count += 1
                        
            print(f"   âœ… PERRO: {dog_count:,} imÃ¡genes")
        
        # Balancear dataset
        if max_samples_per_class:
            self._balance_dataset(max_samples_per_class)
        
        print(f"ğŸ¯ Total samples: {len(self.samples):,}")
        
    def _balance_dataset(self, target_size):
        """Balancea el dataset para tener igual nÃºmero de muestras"""
        # Separar por clase
        no_dog_samples = [s for s in self.samples if s[1] == 0]
        dog_samples = [s for s in self.samples if s[1] == 1]
        
        # Tomar target_size de cada clase
        no_dog_balanced = no_dog_samples[:target_size]
        dog_balanced = dog_samples[:target_size]
        
        self.samples = no_dog_balanced + dog_balanced
        
        print(f"âš–ï¸  Dataset balanceado: {len(no_dog_balanced)} no-perro + {len(dog_balanced)} perro")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # Retornar imagen en negro como fallback
            fallback = torch.zeros((3, 224, 224))
            return fallback, label

# ===================================================================
# MODELO BINARIO
# ===================================================================
class BinaryDogClassifier(nn.Module):
    """Modelo binario para clasificaciÃ³n perro/no-perro"""
    
    def __init__(self, pretrained=True):
        super().__init__()
        
        # Base: EfficientNet-B1 (mÃ¡s ligero que B3)
        self.backbone = models.efficientnet_b1(weights='IMAGENET1K_V1' if pretrained else None)
        
        # Obtener dimensiones de features
        num_features = self.backbone.classifier[1].in_features
        
        # Reemplazar clasificador
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(512),
            nn.Dropout(p=0.2),
            nn.Linear(512, 128),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(128),
            nn.Dropout(p=0.1),
            nn.Linear(128, 2)  # Binario: 0=no-perro, 1=perro
        )
        
    def forward(self, x):
        return self.backbone(x)

# ===================================================================
# ENTRENADOR
# ===================================================================
class BinaryTrainer:
    """Entrenador optimizado para clasificaciÃ³n binaria"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        self.controller = TrainingController()  # Control de parada manual
        
        # Configurar optimizador y scheduler
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=0.001,
            weight_decay=0.01,
            eps=1e-8
        )
        
        # Criterio con pesos balanceados
        self.criterion = nn.CrossEntropyLoss()
        
    def setup_scheduler(self, train_loader, epochs):
        """Configura el scheduler OneCycleLR"""
        total_steps = len(train_loader) * epochs
        self.scheduler = OneCycleLR(
            self.optimizer,
            max_lr=0.003,
            total_steps=total_steps,
            pct_start=0.1,
            anneal_strategy='cos',
            div_factor=10,
            final_div_factor=100
        )
        print(f"ğŸ“ˆ OneCycleLR configurado: {total_steps:,} pasos totales")
        
    def train_epoch(self, train_loader, epoch):
        """Entrena una Ã©poca"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Ã‰poca {epoch}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            # Verificar si se solicitÃ³ parada
            if self.controller.should_stop:
                print(f"\nğŸ›‘ Parada solicitada durante entrenamiento - terminando Ã©poca...")
                break
                
            data, target = data.to(self.device), target.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            self.scheduler.step()
            
            # EstadÃ­sticas
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # Actualizar progress bar
            if batch_idx % 10 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'LR': f'{current_lr:.2e}'
                })
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc, current_lr
    
    def validate(self, val_loader, epoch):
        """Valida el modelo"""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in tqdm(val_loader, desc=f"ValidaciÃ³n {epoch}"):
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                
                val_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = 100. * correct / total
        
        return val_loss, val_acc, all_preds, all_targets
    
    def train_model(self, train_loader, val_loader, epochs, save_path='./binary_models', patience=5):
        """Entrenamiento completo"""
        print(f"ğŸš€ INICIANDO ENTRENAMIENTO BINARIO")
        print("=" * 60)
        print(f"ğŸ¯ Ã‰pocas: {epochs}")
        print(f"ğŸ¤– Modelo: EfficientNet-B1")
        print(f"ğŸ’» Dispositivo: {self.device}")
        print()
        
        # Iniciar control de parada manual
        self.controller.start_monitoring()
        
        # Configurar scheduler
        self.setup_scheduler(train_loader, epochs)
        
        # Crear directorio de guardado
        Path(save_path).mkdir(exist_ok=True)
        
        best_val_acc = 0
        patience_counter = 0
        
        try:
            for epoch in range(1, epochs + 1):
                # Verificar si se solicitÃ³ parada
                if self.controller.should_stop:
                    print(f"\nğŸ›‘ ENTRENAMIENTO DETENIDO POR USUARIO EN Ã‰POCA {epoch}")
                    break
                
                print(f"ğŸ“… Ã‰POCA {epoch}/{epochs}")
                print("-" * 40)
                
                # Entrenar
                train_loss, train_acc, current_lr = self.train_epoch(train_loader, epoch)
                
                # Validar
                val_loss, val_acc, val_preds, val_targets = self.validate(val_loader, epoch)
                
                # Guardar mÃ©tricas
                self.train_losses.append(train_loss)
                self.train_accuracies.append(train_acc)
                self.val_losses.append(val_loss)
                self.val_accuracies.append(val_acc)
                
                # Imprimir resultados
                print(f"ğŸ“ˆ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
                print(f"ğŸ“Š Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
                print(f"ğŸ”„ Learning Rate: {current_lr:.2e}")
                
                # Guardar mejor modelo
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    
                    # Guardar modelo
                    model_path = Path(save_path) / 'best_binary_model.pth'
                    torch.save({
                        'model_state_dict': self.model.state_dict(),
                        'accuracy': val_acc,
                        'epoch': epoch,
                        'optimizer_state_dict': self.optimizer.state_dict(),
                    }, model_path)
                
                    print(f"âœ… Mejor modelo guardado: {val_acc:.2f}% accuracy")
                else:
                    patience_counter += 1
                    print(f"â³ Paciencia: {patience_counter}/{patience}")
                
                # Early stopping
                if patience_counter >= patience:
                    print(f"ğŸ›‘ Early stopping en Ã©poca {epoch}")
                    break
                
                print()
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸  Entrenamiento interrumpido manualmente")
        finally:
            # Detener control de parada
            self.controller.stop_monitoring()
        
        # Generar reporte final
        self._generate_report(save_path, best_val_acc, val_preds, val_targets)
        
        return {
            'best_accuracy': best_val_acc,
            'final_epoch': epoch,
            'train_history': {
                'train_losses': self.train_losses,
                'train_accuracies': self.train_accuracies,
                'val_losses': self.val_losses,
                'val_accuracies': self.val_accuracies
            }
        }
    
    def _generate_report(self, save_path, best_accuracy, preds, targets):
        """Genera reporte de entrenamiento"""
        print("ğŸ“Š GENERANDO REPORTE FINAL...")
        
        # GrÃ¡fica de entrenamiento
        plt.figure(figsize=(15, 5))
        
        # Loss
        plt.subplot(1, 3, 1)
        plt.plot(self.train_losses, label='Train Loss', color='blue', alpha=0.7)
        plt.plot(self.val_losses, label='Val Loss', color='red', alpha=0.7)
        plt.title('ğŸ“‰ PÃ©rdida durante Entrenamiento')
        plt.xlabel('Ã‰poca')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Accuracy
        plt.subplot(1, 3, 2)
        plt.plot(self.train_accuracies, label='Train Acc', color='green', alpha=0.7)
        plt.plot(self.val_accuracies, label='Val Acc', color='orange', alpha=0.7)
        plt.title('ğŸ“ˆ PrecisiÃ³n durante Entrenamiento')
        plt.xlabel('Ã‰poca')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Matriz de confusiÃ³n
        plt.subplot(1, 3, 3)
        cm = confusion_matrix(targets, preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['No-Perro', 'Perro'],
                   yticklabels=['No-Perro', 'Perro'])
        plt.title('ğŸ¯ Matriz de ConfusiÃ³n')
        plt.ylabel('Real')
        plt.xlabel('PredicciÃ³n')
        
        plt.tight_layout()
        plt.savefig(Path(save_path) / 'binary_training_report.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Reporte de clasificaciÃ³n
        class_names = ['No-Perro', 'Perro']
        report = classification_report(targets, preds, target_names=class_names)
        
        with open(Path(save_path) / 'binary_classification_report.txt', 'w') as f:
            f.write("ğŸ• REPORTE DE CLASIFICACIÃ“N BINARIA\n")
            f.write("=" * 50 + "\n")
            f.write(f"ğŸ¯ Mejor Accuracy: {best_accuracy:.2f}%\n")
            f.write(f"ğŸ’» Optimizado para: AMD 7800X3D\n")
            f.write(f"ğŸ¤– Arquitectura: EfficientNet-B1\n\n")
            f.write(report)
        
        print(f"âœ… Reporte guardado en {save_path}")

# ===================================================================
# CONFIGURACIÃ“N DE DATOS
# ===================================================================
def get_transforms():
    """Obtiene las transformaciones de datos"""
    
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def create_dataloaders(data_path, train_transform, val_transform, batch_size=16, num_workers=14):
    """Crea los dataloaders optimizados"""
    print("ğŸ”„ CREANDO DATALOADERS BINARIOS...")
    
    # Crear datasets con lÃ­mite para balanceo
    train_dataset = BinaryDogDataset(
        data_path=data_path,
        transform=train_transform,
        max_samples_per_class=15000  # 15k por clase = 30k total
    )
    
    val_dataset = BinaryDogDataset(
        data_path=data_path,
        transform=val_transform,
        max_samples_per_class=3000   # 3k por clase = 6k total
    )
    
    # Crear DataLoaders optimizados para 7800X3D
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers//2,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    
    print(f"âœ… DataLoaders creados:")
    print(f"   ğŸ‹ï¸  Train: {len(train_dataset):,} samples")
    print(f"   âœ… Val: {len(val_dataset):,} samples")
    print(f"   âš™ï¸  Batch size: {batch_size}")
    print(f"   ğŸ‘· Workers: {num_workers}")
    
    return train_loader, val_loader

# ===================================================================
# FUNCIÃ“N PRINCIPAL
# ===================================================================
def main():
    """FunciÃ³n principal de entrenamiento"""
    print("ğŸ• ENTRENADOR BINARIO CANINO")
    print("ğŸš€ Optimizado para AMD 7800X3D")
    print("=" * 80)
    
    # Configurar entorno
    optimize_for_7800x3d()
    
    # ConfiguraciÃ³n
    DATA_PATH = "./DATASETS"
    BATCH_SIZE = 16  # Ã“ptimo para CPU
    EPOCHS = 20
    NUM_WORKERS = 14  # 7800X3D tiene 16 threads, dejar 2 para sistema
    
    # Verificar datos
    if not Path(DATA_PATH).exists():
        print(f"âŒ Directorio de datos no encontrado: {DATA_PATH}")
        return
    
    # Crear modelo
    print("ğŸ¤– Creando modelo...")
    model = BinaryDogClassifier(pretrained=True)
    
    # Configurar device
    device = torch.device('cpu')
    print(f"ğŸ’» Usando dispositivo: {device}")
    
    # Crear trainer
    trainer = BinaryTrainer(model, device)
    
    # Preparar datos
    train_transform, val_transform = get_transforms()
    train_loader, val_loader = create_dataloaders(
        DATA_PATH, train_transform, val_transform, BATCH_SIZE, NUM_WORKERS
    )
    
    # Entrenar
    start_time = time.time()
    results = trainer.train_model(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=EPOCHS,
        save_path='./binary_models',
        patience=5
    )
    
    training_time = time.time() - start_time
    
    # Resultados finales
    print("\nğŸ¯ ENTRENAMIENTO COMPLETADO")
    print("=" * 60)
    print(f"âœ… Mejor accuracy: {results['best_accuracy']:.2f}%")
    print(f"â±ï¸  Tiempo total: {training_time/3600:.1f} horas")
    print(f"ğŸ“Š Ã‰pocas completadas: {results['final_epoch']}")
    print(f"ğŸ’¾ Modelo guardado: ./binary_models/best_binary_model.pth")
    
    return results

if __name__ == "__main__":
    results = main()